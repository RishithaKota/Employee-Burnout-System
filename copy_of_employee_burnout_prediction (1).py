# -*- coding: utf-8 -*-
"""Copy of Employee Burnout Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nxgMlAHZ_HWIqF0II2FEY0kVyyged4pw
"""



"""Employee Burnout Prediction

Employee burnout is a state of physical, emotional and mental exhaustion caused by excessive and prolonged stress. It can have serious consequences on an individual's well-being and can lead to decreased productivity and job performance, in today's fast-paced and constantly connected world, it is increasingly important to recognize and address the signs of burnout in order to maintain the health and well-being of employees.

we will be exploring the use of regression techniques to predict employee burnout. By analyzing a dataset containing various factors that may contribute to burnout such as workload, mental fatigue job and work-life balance, we can develop a model to identify individuals who may be at risk of burnout. By proactively addressing these risk factors, organizations can help prevent burnout and promote the well-being of their employees
"""

#importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle as pickle
import os

data=pd.read_excel("/content/employee_burnout_analysis-AI.xlsx")

data.head()

data.tail()

data.describe()

data.columns.tolist()

data.nunique()

data.info()

data.isnull().sum()

data.isnull().sum().values.sum()

data.corr(numeric_only=True)['Burn Rate'][:-1]

sns.pairplot(data)
plt.show

data = data.dropna()

data.shape

data.dtypes

data = data.drop('Employee ID', axis=1)

z_scores = stats.zscore(data['Mental Fatigue Score'])
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3)
data = data[filtered_entries]

z_scores = stats.zscore(data['Designation'])
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3)
data = data[filtered_entries]

normalizer = MinMaxScaler()
data[['Mental Fatigue Score', 'Resource Allocation']] = normalizer.fit_transform(data[['Mental Fatigue Score', 'Resource Allocation']])

data = data.drop_duplicates()

num_imputer = SimpleImputer(strategy='mean')
data['Mental Fatigue Score'] = num_imputer.fit_transform(data[['Mental Fatigue Score']])

num_imputer = SimpleImputer(strategy='mean')
data['Resource Allocation'] = num_imputer.fit_transform(data[['Resource Allocation']])

print(f"Min date {data['Date of Joining'].min()}")
print(f"Max date {data['Date of Joining'].max()}")
data_month = data.copy()

data_month["Date of Joining"] = data_month['Date of Joining'].astype("datetime64[ns]")
data_month["Date of Joining"].groupby(data_month["Date of Joining"].dt.month).count().plot(kind="bar", xlabel="Month", ylabel="Hired employees")

data_2008 = pd.to_datetime(["2008-01-01"]*len(data))
data["Days"] = data['Date of Joining'].astype("datetime64[ns]").sub(data_2008).dt.days
data.Days

numeric_data = data.select_dtypes(include=['number'])
correlation = numeric_data.corr()['Burn Rate']
print(correlation)

data.corr(numeric_only=True)['Burn Rate'][:]

data = data.drop(['Date of Joining', 'Days'], axis = 1)

data.head()

cat_columns = data.select_dtypes(object).columns
fig, ax = plt.subplots(nrows = 1, ncols=len(cat_columns),figsize=(10,5))
for i, c in enumerate(cat_columns):
    sns.countplot(x=c, data=data, ax=ax[i])
plt.show()

data = pd.get_dummies(data,columns=['Company Type','WFH Setup Available','Gender'], drop_first=True)
data.head()
encoded_columns = data.columns

y = data['Burn Rate']
x = data.drop('Burn Rate', axis = 1)

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size= 0.7, shuffle=True, random_state = 1)

scaler = StandardScaler()
scaler.fit(x_train)
x_train = pd.DataFrame(scaler.transform(x_train), index=x_train.index, columns=x_train.columns)
x_test = pd.DataFrame(scaler.transform(x_test), index=x_test.index, columns=x_test.columns)

import os
import pickle
scaler_filename = '.../models/scaler.pkl'
os.makedirs(os.path.dirname(scaler_filename), exist_ok=True)
with open(scaler_filename, 'wb') as scaler_file:
  pickle.dump(scaler, scaler_file)

x_train

y_train

linear_regression_model = LinearRegression()
linear_regression_model.fit(x_train, y_train)

print("Linear Regression Model Performance Metrics:\n")
y_pred = linear_regression_model.predict(x_test)

mse=mean_squared_error(y_test,y_pred)
print("Mean Squared Error:", mse)

rmse=mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Sqyared Error:", rmse)

mae= mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

r2=r2_score(y_test,y_pred)
print("R-squarred Score:", r2)

"""How would sales be predicted for a new set of advertising expenditures: $200 on TV, $40 on Radio, and $50 on Newspaper?

"""